# æ•°æ®å¤„ç†æ¨¡å— (Data Processor)

æŠ•èµ„æƒ…æŠ¥æœç´¢å¼•æ“çš„æ ¸å¿ƒæ•°æ®å¤„ç†æ¨¡å—ï¼Œè´Ÿè´£æ•°æ®æ¸…æ´—ã€NLPå¤„ç†ã€å‘é‡åŒ–ã€ç´¢å¼•å’Œå¢é‡æ›´æ–°ã€‚

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

### æ ¸å¿ƒåŠŸèƒ½
- **æ•°æ®æ¸…æ´—ç®¡é“**: å»é‡ã€æ ¼å¼åŒ–ã€å†…å®¹æå–ã€è´¨é‡è¯„ä¼°
- **ä¸­æ–‡NLPå¤„ç†**: åˆ†è¯ã€å®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»
- **æ–‡æœ¬å‘é‡åŒ–**: ä½¿ç”¨ä¸­æ–‡é‡‘èBERTæ¨¡å‹ç”Ÿæˆæ–‡æ¡£å‘é‡
- **Elasticsearchç´¢å¼•**: å»ºç«‹æœç´¢ç´¢å¼•å’Œæ˜ å°„ç­–ç•¥
- **å‘é‡å­˜å‚¨**: Qdrantå‘é‡æ•°æ®åº“é›†æˆ
- **å¢é‡æ›´æ–°**: æ™ºèƒ½æ•°æ®åŒæ­¥å’Œæ›´æ–°æœºåˆ¶
- **æœç´¢å¼•æ“**: å¤šç§æœç´¢ç±»å‹æ”¯æŒï¼ˆæ–‡æœ¬ã€è¯­ä¹‰ã€æ··åˆï¼‰

### æŠ€æœ¯æ ˆ
- **Python 3.8+**
- **Elasticsearch 8.11+**: å…¨æ–‡æœç´¢å¼•æ“
- **Qdrant**: å‘é‡æ•°æ®åº“
- **Transformers**: ä¸­æ–‡NLPæ¨¡å‹
- **Jieba**: ä¸­æ–‡åˆ†è¯
- **Celery**: å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
- **Redis**: ç¼“å­˜å’Œæ¶ˆæ¯é˜Ÿåˆ—
- **Pandas/NumPy**: æ•°æ®å¤„ç†

## ğŸ—ï¸ æ¨¡å—ç»“æ„

```
data-processor/
â”œâ”€â”€ __init__.py                 # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ config.py                   # é…ç½®ç®¡ç†
â”œâ”€â”€ main.py                     # ä¸»ç®¡ç†å™¨
â”œâ”€â”€ pipeline.py                 # æ•°æ®å¤„ç†ç®¡é“
â”œâ”€â”€ demo.py                     # åŠŸèƒ½æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ test_pipeline.py           # ç®¡é“æµ‹è¯•éªŒè¯
â”œâ”€â”€ requirements.txt           # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ README.md                  # æœ¬æ–‡æ¡£
â”‚
â”œâ”€â”€ processors/                # æ•°æ®å¤„ç†å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cleaner.py            # æ•°æ®æ¸…æ´—
â”‚   â”œâ”€â”€ extractor.py          # å†…å®¹æå–
â”‚   â”œâ”€â”€ deduplicator.py       # å»é‡å¤„ç†
â”‚   â””â”€â”€ quality_assessor.py   # è´¨é‡è¯„ä¼°
â”‚
â”œâ”€â”€ nlp/                      # NLPå¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ nlp_processor.py      # NLPç»Ÿä¸€æ¥å£
â”‚   â”œâ”€â”€ segmentation.py       # ä¸­æ–‡åˆ†è¯
â”‚   â”œâ”€â”€ entity_recognition.py # å®ä½“è¯†åˆ«
â”‚   â”œâ”€â”€ sentiment_analysis.py # æƒ…æ„Ÿåˆ†æ
â”‚   â””â”€â”€ text_classifier.py    # æ–‡æœ¬åˆ†ç±»
â”‚
â”œâ”€â”€ vector/                   # å‘é‡å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_manager.py     # å‘é‡ç®¡ç†å™¨
â”‚   â”œâ”€â”€ embeddings.py         # æ–‡æœ¬å‘é‡åŒ–
â”‚   â”œâ”€â”€ vector_store.py       # å‘é‡å­˜å‚¨
â”‚   â””â”€â”€ similarity_search.py  # ç›¸ä¼¼åº¦æœç´¢
â”‚
â”œâ”€â”€ elasticsearch/            # Elasticsearchæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ index_manager.py      # ç´¢å¼•ç®¡ç†
â”‚   â”œâ”€â”€ document_indexer.py   # æ–‡æ¡£ç´¢å¼•
â”‚   â””â”€â”€ search_engine.py      # æœç´¢å¼•æ“
â”‚
â””â”€â”€ incremental/             # å¢é‡æ›´æ–°æ¨¡å—
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ sync_manager.py       # åŒæ­¥ç®¡ç†å™¨
    â”œâ”€â”€ change_detector.py    # å˜æ›´æ£€æµ‹
    â””â”€â”€ incremental_processor.py # å¢é‡å¤„ç†
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd data-processor
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒ

ç¡®ä¿ä»¥ä¸‹æœåŠ¡æ­£åœ¨è¿è¡Œï¼š
- Elasticsearch (ç«¯å£: 9200)
- Qdrant (ç«¯å£: 6333)
- Redis (ç«¯å£: 6379)

### 3. è¿è¡Œæ¼”ç¤º

```bash
python demo.py
```

æ¼”ç¤ºè„šæœ¬å°†å±•ç¤ºï¼š
- ç³»ç»Ÿåˆå§‹åŒ–
- æ•°æ®å¤„ç†ç®¡é“
- æœç´¢åŠŸèƒ½
- ç³»ç»Ÿç»Ÿè®¡

### 4. è¿è¡Œå®Œæ•´æµ‹è¯•

```bash
python test_pipeline.py
```

æµ‹è¯•è„šæœ¬å°†éªŒè¯ï¼š
- ç³»ç»Ÿåˆå§‹åŒ–
- å®Œæ•´æ•°æ®å¤„ç†ç®¡é“
- æœç´¢åŠŸèƒ½
- ç»„ä»¶å¥åº·çŠ¶æ€

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from main import data_processor_manager

async def process_documents():
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    init_result = await data_processor_manager.initialize()
    if not init_result['success']:
        print("åˆå§‹åŒ–å¤±è´¥")
        return
    
    # 2. å‡†å¤‡æ–‡æ¡£æ•°æ®
    documents = [
        {
            'title': 'æ–°é—»æ ‡é¢˜',
            'content': 'æ–°é—»å†…å®¹...',
            'url': 'https://example.com/news/1',
            'source': 'æ–°é—»æ¥æº',
            'publish_time': '2024-10-30T09:30:00Z'
        }
    ]
    
    # 3. å¤„ç†æ–‡æ¡£
    result = await data_processor_manager.process_documents_full_pipeline(
        documents=documents,
        enable_elasticsearch=True,
        enable_vector_store=True
    )
    
    if result['success']:
        print(f"å¤„ç†æˆåŠŸ: {result['processed_documents']} ä¸ªæ–‡æ¡£")
    
    # 4. æœç´¢æ–‡æ¡£
    search_result = await data_processor_manager.search_documents(
        query="æœç´¢å…³é”®è¯",
        search_type="hybrid",
        limit=10
    )
    
    if search_result['success']:
        documents = search_result['result']['documents']
        print(f"æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")

# è¿è¡Œ
asyncio.run(process_documents())
```

### é«˜çº§é…ç½®

```python
from config import config

# è‡ªå®šä¹‰é…ç½®
config.update({
    'elasticsearch': {
        'host': 'localhost',
        'port': 9200,
        'index_prefix': 'investment_'
    },
    'qdrant': {
        'host': 'localhost',
        'port': 6333,
        'collection_name': 'investment_documents'
    },
    'nlp': {
        'model_name': 'bert-base-chinese',
        'max_length': 512,
        'batch_size': 32
    }
})
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. æ•°æ®å¤„ç†ç®¡é“ (Pipeline)

è´Ÿè´£æ•°æ®çš„æ¸…æ´—ã€æå–ã€å»é‡å’Œè´¨é‡è¯„ä¼°ï¼š

```python
from pipeline import DataProcessingPipeline

pipeline = DataProcessingPipeline()
result = pipeline.process_documents(
    documents,
    enable_deduplication=True,
    enable_quality_filter=True
)
```

**åŠŸèƒ½ç‰¹æ€§**:
- HTMLæ ‡ç­¾æ¸…ç†
- æ–‡æœ¬æ ¼å¼åŒ–
- å†…å®¹å»é‡ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
- æ•°æ®è´¨é‡è¯„åˆ†
- å…ƒæ•°æ®æå–

### 2. NLPå¤„ç†å™¨ (NLP Processor)

æä¾›ä¸­æ–‡æ–‡æœ¬çš„æ·±åº¦è¯­è¨€å¤„ç†ï¼š

```python
from nlp.nlp_processor import NLPProcessor

nlp = NLPProcessor()
result = nlp.process_text("å¾…åˆ†æçš„ä¸­æ–‡æ–‡æœ¬")

# ç»“æœåŒ…å«ï¼š
# - segments: åˆ†è¯ç»“æœ
# - entities: å‘½åå®ä½“
# - sentiment: æƒ…æ„Ÿåˆ†æ
# - category: æ–‡æœ¬åˆ†ç±»
```

**æ”¯æŒçš„NLPä»»åŠ¡**:
- **ä¸­æ–‡åˆ†è¯**: ä½¿ç”¨Jiebaåˆ†è¯å™¨
- **å‘½åå®ä½“è¯†åˆ«**: è¯†åˆ«äººåã€åœ°åã€æœºæ„åç­‰
- **æƒ…æ„Ÿåˆ†æ**: æ­£é¢/è´Ÿé¢/ä¸­æ€§æƒ…æ„Ÿåˆ¤æ–­
- **æ–‡æœ¬åˆ†ç±»**: æ–°é—»ã€å…¬å‘Šã€ç ”æŠ¥ç­‰ç±»åˆ«åˆ†ç±»

### 3. å‘é‡ç®¡ç†å™¨ (Vector Manager)

å¤„ç†æ–‡æœ¬å‘é‡åŒ–å’Œç›¸ä¼¼åº¦æœç´¢ï¼š

```python
from vector.vector_manager import VectorManager

vector_mgr = VectorManager()

# ç”Ÿæˆæ–‡æ¡£å‘é‡
embeddings = vector_mgr.generate_embeddings(["æ–‡æœ¬1", "æ–‡æœ¬2"])

# å­˜å‚¨å‘é‡
vector_mgr.store_vectors(documents, embeddings)

# å‘é‡æœç´¢
results = vector_mgr.search(
    query="æœç´¢æŸ¥è¯¢",
    search_type="semantic",
    limit=10
)
```

**å‘é‡åŒ–ç‰¹æ€§**:
- **ä¸­æ–‡é‡‘èBERT**: ä¸“é—¨é’ˆå¯¹é‡‘èé¢†åŸŸä¼˜åŒ–
- **å¤šç»´å‘é‡**: æ”¯æŒ768ç»´å‘é‡è¡¨ç¤º
- **æ‰¹é‡å¤„ç†**: é«˜æ•ˆçš„æ‰¹é‡å‘é‡ç”Ÿæˆ
- **ç›¸ä¼¼åº¦æœç´¢**: ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

### 4. Elasticsearché›†æˆ

æä¾›å¼ºå¤§çš„å…¨æ–‡æœç´¢èƒ½åŠ›ï¼š

```python
from es_indexing.search_engine import SearchEngine

search_engine = SearchEngine()

# å…¨æ–‡æœç´¢
results = search_engine.search(
    query="æœç´¢å…³é”®è¯",
    size=10,
    filters={'category': 'news'}
)

# èšåˆæŸ¥è¯¢
agg_results = search_engine.aggregate(
    field="category",
    size=10
)
```

**æœç´¢ç‰¹æ€§**:
- **ä¸­æ–‡åˆ†æå™¨**: IKåˆ†è¯å™¨æ”¯æŒ
- **å¤šå­—æ®µæœç´¢**: æ ‡é¢˜ã€å†…å®¹ã€æ ‡ç­¾ç­‰
- **è¿‡æ»¤å’Œèšåˆ**: å¤æ‚æŸ¥è¯¢æ”¯æŒ
- **é«˜äº®æ˜¾ç¤º**: æœç´¢ç»“æœé«˜äº®

### 5. å¢é‡æ›´æ–°æœºåˆ¶

æ™ºèƒ½æ£€æµ‹å’Œå¤„ç†æ•°æ®å˜æ›´ï¼š

```python
from incremental.sync_manager import SyncManager

sync_mgr = SyncManager()

# æ£€æµ‹å˜æ›´
changes = sync_mgr.detect_changes(source_data)

# å¢é‡å¤„ç†
sync_mgr.process_incremental_updates(changes)

# åŒæ­¥çŠ¶æ€
status = sync_mgr.get_sync_status()
```

**å¢é‡æ›´æ–°ç‰¹æ€§**:
- **å˜æ›´æ£€æµ‹**: åŸºäºå†…å®¹å“ˆå¸Œçš„æ™ºèƒ½æ£€æµ‹
- **æ‰¹é‡æ›´æ–°**: é«˜æ•ˆçš„æ‰¹é‡æ•°æ®åŒæ­¥
- **å†²çªè§£å†³**: è‡ªåŠ¨å¤„ç†æ•°æ®å†²çª
- **çŠ¶æ€è·Ÿè¸ª**: è¯¦ç»†çš„åŒæ­¥çŠ¶æ€è®°å½•

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å¤„ç†æ€§èƒ½
- **æ–‡æ¡£å¤„ç†é€Ÿåº¦**: ~100-500 æ–‡æ¡£/ç§’
- **NLPå¤„ç†é€Ÿåº¦**: ~50-200 æ–‡æ¡£/ç§’
- **å‘é‡ç”Ÿæˆé€Ÿåº¦**: ~20-100 æ–‡æ¡£/ç§’
- **ç´¢å¼•å†™å…¥é€Ÿåº¦**: ~200-1000 æ–‡æ¡£/ç§’

### æœç´¢æ€§èƒ½
- **ElasticsearchæŸ¥è¯¢**: <100ms (å¹³å‡)
- **å‘é‡æœç´¢**: <200ms (å¹³å‡)
- **æ··åˆæœç´¢**: <300ms (å¹³å‡)

### å‡†ç¡®ç‡æŒ‡æ ‡
- **å»é‡å‡†ç¡®ç‡**: >95%
- **æƒ…æ„Ÿåˆ†æå‡†ç¡®ç‡**: >90%
- **å®ä½“è¯†åˆ«å‡†ç¡®ç‡**: >85%
- **æœç´¢ç›¸å…³æ€§**: >80%

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### æ—¥å¿—é…ç½®

```python
from loguru import logger

# é…ç½®æ—¥å¿—çº§åˆ«
logger.add("data_processor.log", level="INFO")

# æŸ¥çœ‹å¤„ç†ç»Ÿè®¡
stats = data_processor_manager.get_system_statistics()
print(json.dumps(stats, indent=2, ensure_ascii=False))
```

### å¥åº·æ£€æŸ¥

```python
# æ£€æŸ¥ç»„ä»¶å¥åº·çŠ¶æ€
health = await data_processor_manager._check_components_health()

for component, status in health.items():
    print(f"{component}: {status['status']}")
```

### æ€§èƒ½ç›‘æ§

```python
# è·å–å¤„ç†ç»Ÿè®¡
pipeline_stats = pipeline.get_processing_statistics()
nlp_stats = nlp_processor.get_processing_statistics()
vector_stats = vector_manager.get_statistics()
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Elasticsearchè¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥ElasticsearchçŠ¶æ€
   curl -X GET "localhost:9200/_cluster/health"
   ```

2. **Qdrantè¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥QdrantçŠ¶æ€
   curl -X GET "localhost:6333/collections"
   ```

3. **å†…å­˜ä¸è¶³**
   - å‡å°‘æ‰¹å¤„ç†å¤§å°
   - å¢åŠ ç³»ç»Ÿå†…å­˜
   - ä½¿ç”¨æµå¼å¤„ç†

4. **å¤„ç†é€Ÿåº¦æ…¢**
   - æ£€æŸ¥ç¡¬ä»¶èµ„æº
   - ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
   - å¯ç”¨å¹¶è¡Œå¤„ç†

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# è¿è¡Œå•æ­¥è°ƒè¯•
result = pipeline.process_documents(
    documents,
    debug=True,
    verbose=True
)
```

## ğŸ“ˆ æ‰©å±•å’Œå®šåˆ¶

### æ·»åŠ è‡ªå®šä¹‰å¤„ç†å™¨

```python
from processors.base import BaseProcessor

class CustomProcessor(BaseProcessor):
    def process(self, document):
        # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
        return processed_document

# æ³¨å†Œåˆ°ç®¡é“
pipeline.add_processor(CustomProcessor())
```

### è‡ªå®šä¹‰NLPæ¨¡å‹

```python
from nlp.base import BaseNLPModel

class CustomNLPModel(BaseNLPModel):
    def analyze(self, text):
        # è‡ªå®šä¹‰NLPåˆ†æ
        return analysis_result

# æ³¨å†Œæ¨¡å‹
nlp_processor.register_model('custom', CustomNLPModel())
```

### æ‰©å±•æœç´¢åŠŸèƒ½

```python
from elasticsearch.base import BaseSearchEngine

class CustomSearchEngine(BaseSearchEngine):
    def custom_search(self, query, **kwargs):
        # è‡ªå®šä¹‰æœç´¢é€»è¾‘
        return search_results
```

## ğŸ“š APIå‚è€ƒ

è¯¦ç»†çš„APIæ–‡æ¡£è¯·å‚è€ƒå„æ¨¡å—çš„docstringå’Œç±»å‹æ³¨è§£ã€‚

### ä¸»è¦æ¥å£

- `DataProcessorManager`: ä¸»ç®¡ç†å™¨
- `DataProcessingPipeline`: æ•°æ®å¤„ç†ç®¡é“
- `NLPProcessor`: NLPå¤„ç†å™¨
- `VectorManager`: å‘é‡ç®¡ç†å™¨
- `SearchEngine`: æœç´¢å¼•æ“
- `SyncManager`: åŒæ­¥ç®¡ç†å™¨

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»ºPull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚è¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·åˆ›å»ºIssueæˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚

---

**æŠ•èµ„æƒ…æŠ¥æœç´¢å¼•æ“æ•°æ®å¤„ç†æ¨¡å—** - ä¸ºæ™ºèƒ½æŠ•èµ„å†³ç­–æä¾›å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›ã€‚
