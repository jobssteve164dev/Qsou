#!/usr/bin/env python3
"""
çˆ¬è™«å¯åŠ¨è„šæœ¬

å¯åŠ¨çœŸå®çš„çˆ¬è™«è¿›è¡Œæ•°æ®é‡‡é›†
"""

import os
import sys
import subprocess
import logging
from datetime import datetime
from typing import List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class CrawlerManager:
    """çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self):
        self.crawler_dir = current_dir
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def start_crawler(self, spider_name: str, **kwargs) -> bool:
        """å¯åŠ¨æŒ‡å®šçˆ¬è™«"""
        try:
            self.logger.info(f"å¯åŠ¨çˆ¬è™«: {spider_name}")
            
            # æ„å»ºScrapyå‘½ä»¤
            cmd = [
                'scrapy', 'crawl', spider_name,
                '-L', 'INFO',
                '-s', 'LOG_FILE=logs/scrapy.log'
            ]
            
            # æ·»åŠ è‡ªå®šä¹‰å‚æ•°
            for key, value in kwargs.items():
                cmd.extend(['-a', f'{key}={value}'])
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd,
                cwd=self.crawler_dir,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            
            if result.returncode == 0:
                self.logger.info(f"çˆ¬è™« {spider_name} æ‰§è¡ŒæˆåŠŸ")
                self.logger.info(f"è¾“å‡º: {result.stdout}")
                return True
            else:
                self.logger.error(f"çˆ¬è™« {spider_name} æ‰§è¡Œå¤±è´¥")
                self.logger.error(f"é”™è¯¯: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(e)}")
            return False
    
    def start_all_crawlers(self) -> bool:
        """å¯åŠ¨æ‰€æœ‰çˆ¬è™«"""
        spiders = ['financial_news', 'company_announcement']
        success_count = 0
        
        for spider in spiders:
            if self.start_crawler(spider):
                success_count += 1
        
        self.logger.info(f"æˆåŠŸå¯åŠ¨ {success_count}/{len(spiders)} ä¸ªçˆ¬è™«")
        return success_count == len(spiders)
    
    def check_dependencies(self) -> bool:
        """æ£€æŸ¥ä¾èµ–"""
        try:
            # æ£€æŸ¥Scrapy
            result = subprocess.run(
                ['scrapy', '--version'],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                self.logger.error("Scrapyæœªå®‰è£…æˆ–ä¸å¯ç”¨")
                return False
            
            self.logger.info(f"Scrapyç‰ˆæœ¬: {result.stdout.strip()}")
            
            # æ£€æŸ¥æ•°æ®å¤„ç†ç³»ç»Ÿ
            import requests
            try:
                response = requests.get("http://localhost:8001/health", timeout=5)
                if response.status_code == 200:
                    self.logger.info("æ•°æ®å¤„ç†ç³»ç»Ÿè¿æ¥æ­£å¸¸")
                else:
                    self.logger.warning("æ•°æ®å¤„ç†ç³»ç»Ÿå“åº”å¼‚å¸¸")
            except requests.exceptions.RequestException:
                self.logger.warning("æ•°æ®å¤„ç†ç³»ç»Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°å­˜å‚¨")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¾èµ–æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            'logs',
            'downloads',
            'downloads/files',
            'downloads/images'
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            self.logger.info(f"åˆ›å»ºç›®å½•: {directory}")


def main():
    """ä¸»å‡½æ•°"""
    print("=== QsouæŠ•èµ„æƒ…æŠ¥æœç´¢å¼•æ“ - çˆ¬è™«å¯åŠ¨å™¨ ===")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨
    manager = CrawlerManager()
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    manager.create_directories()
    
    # æ£€æŸ¥ä¾èµ–
    if not manager.check_dependencies():
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return False
    
    print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    print()
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    print("è¯·é€‰æ‹©è¦å¯åŠ¨çš„çˆ¬è™«:")
    print("1. è´¢ç»æ–°é—»çˆ¬è™« (financial_news)")
    print("2. å…¬å¸å…¬å‘Šçˆ¬è™« (company_announcement)")
    print("3. å¯åŠ¨æ‰€æœ‰çˆ¬è™«")
    print("4. é€€å‡º")
    
    while True:
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            
            if choice == '1':
                print("\nğŸš€ å¯åŠ¨è´¢ç»æ–°é—»çˆ¬è™«...")
                success = manager.start_crawler('financial_news')
                break
            elif choice == '2':
                print("\nğŸš€ å¯åŠ¨å…¬å¸å…¬å‘Šçˆ¬è™«...")
                success = manager.start_crawler('company_announcement')
                break
            elif choice == '3':
                print("\nğŸš€ å¯åŠ¨æ‰€æœ‰çˆ¬è™«...")
                success = manager.start_all_crawlers()
                break
            elif choice == '4':
                print("ğŸ‘‹ é€€å‡º")
                return True
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-4")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆ")
            return True
        except Exception as e:
            print(f"âŒ è¾“å…¥é”™è¯¯: {str(e)}")
    
    # è¾“å‡ºç»“æœ
    if success:
        print("\nâœ… çˆ¬è™«æ‰§è¡Œå®Œæˆ")
        print("ğŸ“Š æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: logs/crawler.log")
        print("ğŸ“Š æŸ¥çœ‹Scrapyæ—¥å¿—: logs/scrapy.log")
    else:
        print("\nâŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥")
        print("ğŸ“Š æŸ¥çœ‹é”™è¯¯æ—¥å¿—: logs/crawler.log")
    
    return success


if __name__ == '__main__':
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)
