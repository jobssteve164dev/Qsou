# 投资情报搜索引擎法律合规指南

## 1. 数据抓取合规要求

### 1.1 robots.txt协议遵循
- **强制要求**: 所有爬虫必须首先检查并严格遵循目标网站的robots.txt文件
- **实施方案**: 
  - 在Scrapy中启用RobotsTxtMiddleware
  - 设置ROBOTSTXT_OBEY = True
  - 定期更新robots.txt缓存（每24小时）

### 1.2 访问频率控制
- **合理频率**: 每个域名最大访问频率不超过1次/秒
- **实施方案**:
  - 配置DOWNLOAD_DELAY = 1
  - 使用RANDOMIZE_DOWNLOAD_DELAY随机化延迟
  - 实施AutoThrottle自动调节机制

### 1.3 网站服务条款遵循
- **审查机制**: 建立目标网站服务条款定期审查流程
- **白名单策略**: 仅抓取明确允许或公开可访问的数据源
- **风险评估**: 每个新增数据源都需要进行法律风险评估

## 2. 数据使用合规规范

### 2.1 公开信息原则
- **仅抓取公开信息**: 不抓取需要登录或付费的内容
- **来源标注**: 所有数据必须标注原始来源
- **版权尊重**: 不完整复制受版权保护的内容

### 2.2 数据处理合规
- **个人信息保护**: 自动识别并过滤个人敏感信息
- **数据匿名化**: 对可能包含个人信息的数据进行匿名化处理
- **存储期限**: 建立数据生命周期管理，定期清理过期数据

## 3. 技术合规措施

### 3.1 用户代理标识
- **真实标识**: 使用能准确标识爬虫身份的User-Agent
- **联系信息**: 在User-Agent中包含联系邮箱以便网站管理员联系
- **示例**: "Qsou-InvestmentBot/1.0 (+http://your-domain.com/bot; contact@your-domain.com)"

### 3.2 请求头规范
- **模拟真实浏览器**: 设置完整的HTTP头信息
- **尊重缓存**: 正确处理HTTP缓存机制
- **条件请求**: 支持If-Modified-Since等条件请求头

### 3.3 错误处理与重试
- **优雅降级**: 遇到403/429错误时立即停止访问该域名
- **退避算法**: 实施指数退避重试策略
- **错误记录**: 详细记录所有访问异常以便审查

## 4. 数据源分类管理

### 4.1 官方数据源（低风险）
- 上交所、深交所公告信息
- 证监会公开信息
- 各大银行公开财报
- 政府部门公开数据

### 4.2 媒体数据源（中等风险）
- 财经媒体新闻（需确认使用条款）
- 行业研究报告（仅摘要部分）
- 公开分析文章（标注来源）

### 4.3 禁止数据源（高风险）
- 付费数据服务
- 需要登录的私有信息
- 明确禁止爬虫的网站
- 个人社交媒体内容

## 5. 监控与审计机制

### 5.1 合规监控系统
- **实时监控**: 监控爬虫行为是否符合robots.txt
- **异常报警**: 访问频率超标或被阻止时立即报警
- **日志审计**: 完整记录所有数据访问日志

### 5.2 定期合规审查
- **月度审查**: 每月审查新增数据源的合规性
- **季度评估**: 每季度评估整体合规风险
- **年度审计**: 每年进行完整的合规审计

## 6. 应急响应预案

### 6.1 收到网站方投诉
1. 立即停止对该网站的所有访问
2. 删除已抓取的相关数据
3. 与网站方协商解决方案
4. 记录事件并总结经验教训

### 6.2 法律风险预警
1. 建立法律顾问咨询机制
2. 定期关注相关法律法规变化
3. 及时调整合规策略
4. 建立应急响应团队

## 7. 用户责任与免责声明

### 7.1 用户使用规范
- 明确告知用户数据来源和使用限制
- 要求用户遵守相关法律法规
- 禁止用户将数据用于非法用途

### 7.2 服务免责声明
- 数据准确性免责
- 投资决策风险提示
- 第三方数据来源免责
- 法律变更影响免责

通过严格执行以上合规措施，可以最大程度降低法律风险，确保投资情报搜索引擎的合法合规运行。
