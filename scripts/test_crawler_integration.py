#!/usr/bin/env python3
"""
çˆ¬è™«é›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯•çˆ¬è™«ä¸æ•°æ®å¤„ç†ç³»ç»Ÿçš„é›†æˆ
"""

import os
import sys
import asyncio
import requests
import json
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

def test_api_connection():
    """æµ‹è¯•APIè¿æ¥"""
    print("ğŸ” æµ‹è¯•APIè¿æ¥...")
    
    try:
        # æµ‹è¯•APIç½‘å…³å¥åº·æ£€æŸ¥
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            print("âœ… APIç½‘å…³è¿æ¥æ­£å¸¸")
            return True
        else:
            print(f"âŒ APIç½‘å…³å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"âŒ APIç½‘å…³è¿æ¥å¤±è´¥: {str(e)}")
        return False

def test_data_processing_api():
    """æµ‹è¯•æ•°æ®å¤„ç†API"""
    print("ğŸ” æµ‹è¯•æ•°æ®å¤„ç†API...")
    
    try:
        # æµ‹è¯•æ•°æ®å¤„ç†APIçŠ¶æ€
        response = requests.get("http://localhost:8000/api/v1/process/status", timeout=5)
        if response.status_code == 200:
            status = response.json()
            print("âœ… æ•°æ®å¤„ç†APIæ­£å¸¸")
            print(f"   Celeryè¿æ¥: {status.get('celery_connected', False)}")
            return True
        else:
            print(f"âŒ æ•°æ®å¤„ç†APIå“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"âŒ æ•°æ®å¤„ç†APIè¿æ¥å¤±è´¥: {str(e)}")
        return False

def test_crawler_data_submission():
    """æµ‹è¯•çˆ¬è™«æ•°æ®æäº¤"""
    print("ğŸ” æµ‹è¯•çˆ¬è™«æ•°æ®æäº¤...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_documents = [
        {
            "id": "test_news_001",
            "type": "news",
            "title": "æµ‹è¯•è´¢ç»æ–°é—»æ ‡é¢˜",
            "content": "è¿™æ˜¯ä¸€æ¡æµ‹è¯•è´¢ç»æ–°é—»å†…å®¹ï¼Œç”¨äºéªŒè¯çˆ¬è™«ä¸æ•°æ®å¤„ç†ç³»ç»Ÿçš„é›†æˆã€‚å†…å®¹åŒ…å«æŠ•èµ„ã€è‚¡ç¥¨ã€å¸‚åœºç­‰å…³é”®è¯ã€‚",
            "url": "https://example.com/news/001",
            "source": "test_source",
            "publish_time": datetime.now().isoformat(),
            "author": "æµ‹è¯•ä½œè€…",
            "tags": ["è´¢ç»", "è‚¡ç¥¨", "æŠ•èµ„"],
            "category": "è‚¡ç¥¨",
            "crawl_time": datetime.now().isoformat(),
            "content_length": 50,
            "metadata": {
                "crawler": "test_spider",
                "domain": "example.com",
                "language": "zh-CN"
            }
        },
        {
            "id": "test_announcement_001",
            "type": "announcement",
            "title": "æµ‹è¯•å…¬å¸å…¬å‘Šæ ‡é¢˜",
            "content": "è¿™æ˜¯ä¸€æ¡æµ‹è¯•å…¬å¸å…¬å‘Šå†…å®¹ï¼Œç”¨äºéªŒè¯çˆ¬è™«ä¸æ•°æ®å¤„ç†ç³»ç»Ÿçš„é›†æˆã€‚å†…å®¹åŒ…å«å…¬å¸ã€å…¬å‘Šã€è´¢åŠ¡ç­‰å…³é”®è¯ã€‚",
            "url": "https://example.com/announcement/001",
            "source": "test_exchange",
            "exchange": "æµ‹è¯•äº¤æ˜“æ‰€",
            "company_name": "æµ‹è¯•å…¬å¸",
            "stock_code": "000001",
            "announcement_type": "è´¢åŠ¡æŠ¥å‘Š",
            "publish_time": datetime.now().isoformat(),
            "announcement_id": "TEST001",
            "crawl_time": datetime.now().isoformat(),
            "content_length": 50,
            "is_important": True,
            "metadata": {
                "crawler": "test_spider",
                "domain": "example.com",
                "language": "zh-CN"
            }
        }
    ]
    
    try:
        # æäº¤æµ‹è¯•æ•°æ®
        payload = {
            "documents": test_documents,
            "source": "test_crawler",
            "batch_id": f"test_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "enable_elasticsearch": True,
            "enable_vector_store": True,
            "enable_nlp_processing": True
        }
        
        response = requests.post(
            "http://localhost:8000/api/v1/process/process",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… çˆ¬è™«æ•°æ®æäº¤æˆåŠŸ")
            print(f"   å¤„ç†çŠ¶æ€: {result.get('status')}")
            print(f"   å¤„ç†æ•°é‡: {result.get('processed_count')}")
            print(f"   å¤±è´¥æ•°é‡: {result.get('failed_count')}")
            print(f"   æ‰¹æ¬¡ID: {result.get('batch_id')}")
            return True
        else:
            print(f"âŒ çˆ¬è™«æ•°æ®æäº¤å¤±è´¥: {response.status_code}")
            print(f"   å“åº”å†…å®¹: {response.text}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ çˆ¬è™«æ•°æ®æäº¤å¼‚å¸¸: {str(e)}")
        return False

def test_search_functionality():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•æœç´¢API
        search_payload = {
            "query": "æµ‹è¯•è´¢ç»æ–°é—»",
            "search_type": "hybrid",
            "page": 1,
            "page_size": 10
        }
        
        response = requests.post(
            "http://localhost:8000/api/v1/search/",
            json=search_payload,
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… æœç´¢åŠŸèƒ½æ­£å¸¸")
            print(f"   æœç´¢ç»“æœæ•°é‡: {result.get('total_count', 0)}")
            print(f"   æœç´¢æ—¶é—´: {result.get('search_time_ms', 0)}ms")
            return True
        else:
            print(f"âŒ æœç´¢åŠŸèƒ½å¼‚å¸¸: {response.status_code}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ æœç´¢åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== çˆ¬è™«é›†æˆæµ‹è¯• ===")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # æµ‹è¯•æ­¥éª¤
    tests = [
        ("APIè¿æ¥æµ‹è¯•", test_api_connection),
        ("æ•°æ®å¤„ç†APIæµ‹è¯•", test_data_processing_api),
        ("çˆ¬è™«æ•°æ®æäº¤æµ‹è¯•", test_crawler_data_submission),
        ("æœç´¢åŠŸèƒ½æµ‹è¯•", test_search_functionality)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"ğŸ“‹ {test_name}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            results.append((test_name, False))
        print()
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print()
    print(f"æ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«é›†æˆæ­£å¸¸ï¼")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")
        return False

if __name__ == '__main__':
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)
