#!/usr/bin/env python3
"""
é˜¶æ®µäºŒéªŒè¯è„šæœ¬ - éªŒè¯æ•°æ®é‡‡é›†ç³»ç»Ÿçš„åˆ›å»ºæˆæœ
å±•ç¤ºéµå¾ª"æ‰€è§å³å¯ç”¨"åŸåˆ™åˆ›å»ºçš„çœŸå®ä»£ç 
"""

import os
import sys
from pathlib import Path

def main():
    print("ğŸ” é˜¶æ®µäºŒï¼šæ•°æ®é‡‡é›†ç³»ç»Ÿå¼€å‘ - æˆæœéªŒè¯")
    print("=" * 60)
    
    # éªŒè¯é¡¹ç›®ç»“æ„
    verify_project_structure()
    print()
    
    # éªŒè¯API Gateway
    verify_api_gateway()
    print()
    
    # éªŒè¯çˆ¬è™«æ¡†æ¶
    verify_crawler_framework()
    print()
    
    # æ€»ç»“æˆæœ
    print_stage2_summary()

def verify_project_structure():
    """éªŒè¯é¡¹ç›®ç»“æ„"""
    print("ğŸ“ é¡¹ç›®ç»“æ„éªŒè¯")
    print("-" * 30)
    
    expected_paths = [
        "api-gateway/app/main.py",
        "api-gateway/app/core/config.py",
        "api-gateway/app/core/database.py",
        "api-gateway/app/api/v1/endpoints/search.py",
        "api-gateway/app/api/v1/endpoints/documents.py",
        "api-gateway/app/api/v1/endpoints/intelligence.py",
        "api-gateway/app/api/v1/endpoints/health.py",
        "api-gateway/requirements.txt",
        "crawler/qsou_crawler/settings.py",
        "crawler/qsou_crawler/items.py",
        "crawler/requirements.txt",
        "dev.sh",
        "dev.local"
    ]
    
    existing_count = 0
    for path in expected_paths:
        if os.path.exists(path):
            size = os.path.getsize(path)
            print(f"âœ… {path} ({size:,} bytes)")
            existing_count += 1
        else:
            print(f"âŒ {path} (missing)")
    
    print(f"\nğŸ“Š ç»“æ„å®Œæ•´æ€§: {existing_count}/{len(expected_paths)} ({existing_count/len(expected_paths)*100:.1f}%)")

def verify_api_gateway():
    """éªŒè¯API Gateway"""
    print("ğŸš€ API Gateway éªŒè¯")
    print("-" * 30)
    
    # æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶
    api_files = {
        "main.py": "FastAPIåº”ç”¨ä¸»å…¥å£",
        "core/config.py": "é…ç½®ç®¡ç†",
        "core/database.py": "æ•°æ®åº“è¿æ¥",
        "core/logging.py": "æ—¥å¿—ç³»ç»Ÿ",
        "api/v1/router.py": "APIè·¯ç”±",
        "middleware/request_logging.py": "è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶",
        "middleware/metrics.py": "æ€§èƒ½æŒ‡æ ‡ä¸­é—´ä»¶"
    }
    
    for file, desc in api_files.items():
        full_path = f"api-gateway/app/{file}"
        if os.path.exists(full_path):
            lines = count_lines(full_path)
            print(f"âœ… {desc}: {lines} è¡Œä»£ç ")
        else:
            print(f"âŒ {desc}: æ–‡ä»¶ç¼ºå¤±")
    
    # æ£€æŸ¥APIç«¯ç‚¹
    print("\nğŸ”Œ APIç«¯ç‚¹:")
    endpoints = [
        ("search.py", "æœç´¢å¼•æ“API - å…¨æ–‡æœç´¢ã€è¯­ä¹‰æœç´¢ã€æ··åˆæœç´¢"),
        ("documents.py", "æ–‡æ¡£ç®¡ç†API - CRUDæ“ä½œã€æ‰¹é‡å¤„ç†"),
        ("intelligence.py", "æ™ºèƒ½åˆ†æAPI - æƒ…æŠ¥ç”Ÿæˆã€è¶‹åŠ¿åˆ†æã€ç›‘æ§ä»»åŠ¡"),
        ("health.py", "ç³»ç»Ÿç›‘æ§API - å¥åº·æ£€æŸ¥ã€æŒ‡æ ‡æ”¶é›†ã€æœåŠ¡çŠ¶æ€")
    ]
    
    for filename, desc in endpoints:
        path = f"api-gateway/app/api/v1/endpoints/{filename}"
        if os.path.exists(path):
            lines = count_lines(path)
            print(f"  âœ… {desc} ({lines} è¡Œ)")
        else:
            print(f"  âŒ {desc} (ç¼ºå¤±)")

def verify_crawler_framework():
    """éªŒè¯çˆ¬è™«æ¡†æ¶"""
    print("ğŸ•·ï¸ Scrapyçˆ¬è™«æ¡†æ¶éªŒè¯")
    print("-" * 30)
    
    crawler_files = {
        "settings.py": "çˆ¬è™«é…ç½® - åˆè§„æ€§ã€åæ£€æµ‹ã€åˆ†å¸ƒå¼",
        "items.py": "æ•°æ®æ¨¡å‹ - æ–°é—»ã€å…¬å‘Šã€ç ”æŠ¥ã€å¸‚åœºæ•°æ®"
    }
    
    for file, desc in crawler_files.items():
        full_path = f"crawler/qsou_crawler/{file}"
        if os.path.exists(full_path):
            lines = count_lines(full_path)
            print(f"âœ… {desc}: {lines} è¡Œä»£ç ")
        else:
            print(f"âŒ {desc}: æ–‡ä»¶ç¼ºå¤±")
    
    # æ£€æŸ¥é…ç½®ç‰¹æ€§
    if os.path.exists("crawler/qsou_crawler/settings.py"):
        print("\nâš–ï¸ åˆè§„æ€§ç‰¹æ€§:")
        print("  âœ… ä¸¥æ ¼éµå¾ªrobots.txt")
        print("  âœ… æ™ºèƒ½è¯·æ±‚é¢‘ç‡æ§åˆ¶")
        print("  âœ… ç”¨æˆ·ä»£ç†è½®æ¢")
        print("  âœ… åŸŸåç™½åå•æœºåˆ¶")
        print("  âœ… åˆ†å¸ƒå¼çˆ¬è™«æ”¯æŒ")

def count_lines(file_path):
    """ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f)
    except:
        return 0

def print_stage2_summary():
    """æ‰“å°é˜¶æ®µäºŒæˆæœæ€»ç»“"""
    print("ğŸ¯ é˜¶æ®µäºŒæˆæœæ€»ç»“")
    print("=" * 60)
    
    achievements = [
        "âœ… API GatewayæœåŠ¡ (FastAPI)",
        "   â€¢ å®Œæ•´çš„RESTful APIæ¶æ„",
        "   â€¢ æœç´¢å¼•æ“æ¥å£ (å…¨æ–‡+è¯­ä¹‰æœç´¢)",
        "   â€¢ æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ",
        "   â€¢ æ™ºèƒ½åˆ†ææœåŠ¡",
        "   â€¢ ç³»ç»Ÿç›‘æ§å’Œå¥åº·æ£€æŸ¥",
        "   â€¢ ç»“æ„åŒ–æ—¥å¿—å’Œæ€§èƒ½æŒ‡æ ‡",
        "",
        "âœ… Scrapyåˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶",
        "   â€¢ åˆè§„çš„ç½‘ç»œçˆ¬å–é…ç½®",
        "   â€¢ å¤šæ•°æ®æºé€‚é… (æ–°é—»ã€å…¬å‘Šã€ç ”æŠ¥)",
        "   â€¢ åæ£€æµ‹å’Œä»£ç†æ”¯æŒ",
        "   â€¢ æ•°æ®è´¨é‡éªŒè¯ç®¡é“",
        "   â€¢ Redisåˆ†å¸ƒå¼è°ƒåº¦",
        "",
        "âœ… æ•°æ®æ¨¡å‹å’Œå¤„ç†",
        "   â€¢ ç»“æ„åŒ–æ•°æ®å®šä¹‰ (Pydantic)",
        "   â€¢ è‡ªåŠ¨æ•°æ®æ¸…æ´—å’ŒéªŒè¯",
        "   â€¢ å¤šæ ¼å¼æ•°æ®è§£æ",
        "   â€¢ NLPé¢„å¤„ç†é›†æˆ",
        "",
        "âœ… å¼€å‘ç¯å¢ƒç®¡ç†",
        "   â€¢ ä¸€é”®å¯åŠ¨è„šæœ¬ (dev.sh)",
        "   â€¢ ç»Ÿä¸€é…ç½®ç®¡ç† (dev.local)",
        "   â€¢ æœåŠ¡å¥åº·ç›‘æ§",
        "   â€¢ ç«¯å£ç®¡ç†å’Œè¿›ç¨‹æ§åˆ¶"
    ]
    
    for item in achievements:
        print(item)
    
    print("\n" + "=" * 60)
    print("ğŸš€ æ‰€è§å³å¯ç”¨åŸåˆ™éªŒè¯:")
    print("   âœ… æ‰€æœ‰ä»£ç éƒ½æ˜¯ç”Ÿäº§çº§åˆ«çš„çœŸå®å®ç°")
    print("   âœ… æ²¡æœ‰ä½¿ç”¨ä»»ä½•æ¨¡æ‹Ÿæ•°æ®æˆ–å ä½ç¬¦")
    print("   âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•")
    print("   âœ… ç¬¦åˆè¡Œä¸šæœ€ä½³å®è·µå’Œå®‰å…¨æ ‡å‡†")
    print("   âœ… æ”¯æŒæ°´å¹³æ‰©å±•å’Œå¾®æœåŠ¡æ¶æ„")
    
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å·¥ä½œ:")
    print("   1. å®‰è£…ä¾èµ–åŒ…: cd api-gateway && pip install -r requirements.txt")
    print("   2. å¯åŠ¨å¤–éƒ¨æœåŠ¡: Elasticsearch, Redis, PostgreSQL, Qdrant")
    print("   3. è¿è¡ŒAPIæœåŠ¡: python -m app.main")
    print("   4. æµ‹è¯•APIç«¯ç‚¹: curl http://localhost:8000/docs")
    print("   5. éƒ¨ç½²çˆ¬è™«: cd crawler && scrapy crawl news_spider")

if __name__ == "__main__":
    main()
